+K8s着重于不间断地服务状态（web服务器 | 缓存服务器）和原生云平台应用（Nosql），在不久地的来会支持各种生产云平台地各种服务
在K8s中，所有容器均在Pod中运行，一个Pod可以承载一个或者多个相关容器（昨天就晓得了..）

-------------------------------------

K8s像什么，像管理牲畜一样管理你的服务（运行实例），而不是像宠物一样，同时提高资源的利用率，让开发者关注在应用开发本身，高可用的事情交给K8s就好！
233形象化的解释

shell有三味变量：
1）局部变量 -> 在脚本或命令中定义，仅在当前shell实例中有效，其他shell启动的程序无法访问局部变量
2）环境变量 ->所有的程序，包括shell启动的程旭，都能访问的环境变量，有些程序需要环境变量来保证其正常运行，必要的时候shell脚本也可以定义环境变量
3）shell变量 ->由shell程序设置的特殊变量，shell变量中由一部分是局部变量，一部分是环境变量，这些变量都保证了shell的正常运行

shell中最常用最有效的数据类型是字符串，单引号双引号或不用引号均可
①单引号：限制：单引号字符串会原样输出，单引号字符串中的变量是无效的；不能单独一个引号，但可以作为字符串拼接使用成对出现。
②双引号：优点：双引号中可以有变量，可以出现转义字符
③拼接字符串：

获取字符串长度
string = " abcd "
echo ${ #string }

提取子字符串
string = " runoob is a greet site "
echo ${ string:1:4 }  #会输出unoo

查找子字符串位置
echo ` expr index " $string " io ` #查找字符串i或o的位置（那个线出现就计算哪个）

Shell数组：
定义数组    在Shell中，用括号来表示数组，数组元素用“空格”符号分隔开，定义数组的一般形式为：
数组名 = (值1 值2 值3 ... 值n)
此外还可以单独定义数组的各个分量
xxx[y] = z

shell参数传递：
$* 与 $@
共同点：都是引用所有参数
不同只有在双引号下才会体现出来，假设脚本运行时写了三个参数1、2、3，则
“ * ” 等价于 "1 2 3" (传递了一个参数)
而 “ @ ”等价于 “1” “2” “3” (传递了三个参数)

关于数组建立的方法runoob的教程与实际虚拟机的应用不太一样

Redis中支持五种数据类型：
①String字符串    
string类型是redis最基本的类型，可以以及为与Memcached一摸一样的类型，一个key对应一个value。string类型是二进制安全的，意思是redis的string可以包含任何数据，比如jpg图片或是序列化的对象。最基本类型，最大存储512MB
SET name "runoob"
GET name
> “runoob”
②hash哈希    
是一个键值对集合（key=>value）
是一个string类型的field类型的field和value的映射表，hash特别使用用于存储对象
HMSET runoob field1 "Hello" field "World"
HGET runoob field1
> "Hello"
③list列表
Redis列表是简单的字符串列表，按照插入顺序排列。可以添加一个元素到列表头部或是尾部
lpush runoob redis
lpush runoob mongodb
lpush runoob rabitmq
lrange runoob 0 10
> "rabitmq"
>"mongodb"
>"redis"
④set集合    
是string类型的无序集合。集合是通过哈希表实现的，所以添加删除查找的复杂度都是O（1）
⑤zset有序集合
zset和set一样也是string类型元素的集合，不允许重复，每个元素都会关联一个double类型的分数。redis通过分数来为集合中的成员进行从小到大的排序。zset的成员是唯一的，但分数（score）却可以重复

HyperLogLog用于基数统计的算法，优点

Apache Spark 一种基于Hadoop Distributed File System（HDFS）的并行计算框架。与MapReduce不同，SPark并不局限于编写map和reduce两个方法，提供了更为强大的内存计算模型（in-memory computing），使得用户可以通过编程将数据读取到集群的内存当中，并且可以方便用户快速地重复查询，非常适合用于机器学习算法。
允许用户读取、转换、聚合诗句，还可以轻松地训练和部署复杂地统计模型

①任何Spark应用程序都会分离主节点上的单个驱动程序(可以包好多个作业)，然后将执行进程（包含多个任务）分配给多个工作节点
驱动进程会确定任务进程的数量和组成，这些任务进程是根据为指定作业生成分配给执行节点的。
注意，任何工作节点都可以执行来自多个不同作业的多个任务，
Spark作业与一系列对象依赖关联，依赖关系是有向无环图


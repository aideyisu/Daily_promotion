
Spark中的三种数据结构
共同点：都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利
①RDD：是Spark提供的最重要的抽象的概念，弹性是分布式数据集。允许程序员以容错的方式在大型集群上执行内存计算
②Dataframe：以列的形式组织起来，类似于关系数据库中的表。是一个不可变的数据集合。弥补了RDD的缺点，但同时失去了RDD的优点
③dataset：目的是结合RDD的好处

Ps 大概理解一下什么是弹性，如果数据集一部分丢失，则可以对他们进行重建。具有自动容错、位置感知调度和可伸缩性。容错性最难以实现，大部分分布式数据集的容错性有两种实现方法：数据检查点和记录数据的更新。对于大规模数据分析系统，数据检查点操作成本高，beacuse 大规模数据在服务器之间传输可能会带来各方面的问题，相比于记录数据的更新，RDD也只支持粗粒度转换，可就是记录如何从其他RDD转换而来（lineage），以便恢复丢失的分区
总之分布式弹性数据集的特性下：
①数据结构不可变    ②支持跨集群的分布式数据操作    ③可以对数据记录按key进行区分    
④提供了粗粒度转换操作    ⑤数据存储在内存中，保证了低延迟
RDD的全称为 Resilient Distributed Datasets 弹性分布式数据集

    Pss 粗粒度 & 细粒度
一个相对的概念，不是说所有这几粗细的工作都是一致偏向的。粗粒度与细粒度的主要，是处于重用的目的。就像类的设计，为了尽可能重用，所以采用细粒度的设计模式，将一个复杂的类（粗粒度）拆分成高度重用的职责清晰的类（细粒度）。对于数据库的设计，原则是尽量减少表的数量and表与表之间的链接，能够设计成一个表就无需细分，可以考虑粗粒度方式
在数据库中通常把数据库表或者由基本表导出的视图中的某个层称为粗粒度的控制访问，而细粒度控制规则则是把安全控制细化到数据库的列 || 行级

Spark中的Lineage血统
说明：RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage记录下来，仪表恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分数据丢失时，它可以根据这些信息来重新运算有和恢复丢失的数据分区
体现过程：RDD在计算过程中，如果有一个RDD的分区信息丢失，该RDD会首先判断是否有缓存，有缓存则直接取出缓存的数据，没有则判断是否做过checkpoint，如果没有checkpoint，则从父RDD分区开始重新计算，其他分区都不用重新计算，这样既保证了容错性，又提高了运算效率
区别：在任务计算过程中，如果其中过一个Executor宕掉了，worker会重新启动一个心得executor继续完成剩余的任务，如果某一个WOrker宕掉了，此时的master不会重新启动心得worker，会把宕掉的worker没有完成的任务重新分配给其他worker进行计算，这个过程和lineage没有关系，属于集群的容错机制。

众所周知（我不知）
Spark主要针对于两种场景：
①机器学习，数据挖掘，图应用中常用的迭代算法（每一次迭代对数据执行相似的函数）
②交互式数据挖掘工具（用户反复查询一个数据子集）

MapReduce等框架不明确支持迭代中间结果/数据子集的共享，所以需要将数据输出到磁盘，然后在每次查询时重新加载，这将带来比较大的开销。
那么既然反复读写磁盘和从磁盘加载数据使得性能下降，那么我们就把数据放入内存中，这就是Spark基于内存的弹性分布式数据集的出发点

虽然只支持粗粒度转换限制了编程模型，但是RDD仍可以适用于很多应用，特别是数据并行的批量分析应用，包括数据挖掘，级去学习，图算法等。因为这些程序通常会在很多记录上执行同样的操作

RDD是只读的、区分记录的集合。RDD只能基于稳定物理存储中的数据集和其他已有的RDD上执行确定性操作来创建。这些确定性操作称之为转换

DataFrame按列名存储没类似于关系型数据库中的表，DataFrame的设计是为了让大数据处理起来更容易。DataFrame允许开发者把结构化数据集导入DataFrame，并做了higher-level抽象，DataFrame提供特定领域的语言API来操作数据集

Datasets具有两个完全不同的API特征：强类型(strongly-typed)API和弱类型(untyped)API
对Spark开发者而言，Spark2.0 Dataframe和Dataset的同一API获得以下好处：
①静态类型(Static-typing)和运行时类型安全(runtime type-safety)
静态类型和运行时的类型安全是一个范围，对SQL限制很少，但是Dataset限制很多
Spark SQL查询语句直到运行时才能发现语法错误，代价很大
但是Dataset和Dataframe在编译时就可以捕捉到错误，节约开发时间和成本。
Data对开发人员的限制是最大的
②High-level抽象以及结构化和半结构化数据集的自定义视图
我们大多数使用结构化数据的人都习惯于以柱状方式查看和处理数据，或者访问对象中的特定属性。将Dataset作为Dataset [ ElementType ]类型对象的集合，可以无缝获得JVM对象的编译时安全性和自定义视图，可以使用高级方法轻松地显示or处理上面代码中生成的强类型数据集
③简单易用的API
虽然结构化数据会给Spark程序操作数据集带来很多限制，但它特引入了丰富的语义和一组简单的特定操作。大部分计算可以被Dataset的high-level API所支持。

---------------------------------------------------------------------------

spark的基础装好了,,,,辗转以下决定看pipenv的概念
pipenv。
首先会py的同学应该都了解pip这个工具，绝大部分的第三方库都可以用pip来安装，用起来比较方便

但是，当我们要把项目部署到服务器上时，就会有一些麻烦了。加入项目用到很多包的话，一个个安装会比较麻烦，而且没有通用性。
pipenv可以帮助我们管理Python和第三方库的版本
如果是在项目中第一次运行pipenv命令的话，会在项目中创建一个名为pipfile的文件，文件内容类似于下面：

[[ source ]]
url = " https://pypi.org/simple "
verify_ssl = true
name = "pypi"

[ packages ]
requests-html = " * "

[ dev-packages ]

[ requires ]
python_version = "3.7"
其中如果运行过install、update等命令的话，还会创建一个Pipfile.lock文件，类似于npm中的lock文件。

这两个文件就是pipenv用于管理第三方库的配置文件，如果同时使用版本控制软件的话，需要将他们也加入进入

例如，我想在项目中安装requests这个包，可以运行 pipenv install requests
还可以指定具体版本号 pipenv install requests==2.13.0
当你第一次运行pipenv的时候，会先创建pipfile文件，否则会修改pipfile文件
该命令还有一个常用参数 -d 或 --dev，用于安装仅供开发的包

卸载：pipenv uninstall requests    该命令还有两个参数 --all 和 --all-dev 用于卸载所有包和所有开发包
更新：
pipenv update --outdated（查看所有需要更新的包）
pipenv update （更新所有包）
pipenv update  <包名>

如果项目中有 requirements.txt 文件，pipenv会在安装的时候自动导入。如果需要导入其他位置的
requirements.txt ，可以用下面的指令
pipenv install -r path/to/requirements.txt

同时可以指定Py版本
pipenv会创建虚拟Python环境，并在其中用pip安装所有包。同时也可以指定Py版本
pipenv --python 3
pipenv --python 3.6
pipenv --python 2.7.14
三种版本号都支持
如果不指定版本号，pipenv会使用系统默认的Py版本。这里指定的Py必须是系统已经安装的并可以在环境变量中搜索到的版本号，但如果未指定版本，会提示错误。

启动一个在虚拟环境中的shell
pipenv shell
但如果不想启动shell，而是直接在虚拟环境中直接执行命令，可以使用run

========================================




